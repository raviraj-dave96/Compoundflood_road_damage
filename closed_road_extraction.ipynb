{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Define input parameters\n",
    "# Define input paths (Replace these with actual paths)\n",
    "flood_folder = r\"./data/flood_scenarios\"  # Folder containing flood shapefiles\n",
    "output_folder = r\"./output/closed_roads\"  # Folder to save extracted road data\n",
    "road_shapefile = r\"./data/network/road_network.shp\"  # Road network shapefile\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define filename pattern for flood files\n",
    "base_filename = \"flood_scenario_\"\n",
    "\n",
    "# Load road shapefile\n",
    "road_gdf = gpd.read_file(road_shapefile)\n",
    "\n",
    "# Function to Process Each Flood Shapefile\n",
    "def process_flood_file(index):\n",
    "    suffix = f\"{index:02d}\"  # Format index as two-digit number\n",
    "    flood_file = f\"{base_filename}{suffix}.shp\"\n",
    "    flood_path = os.path.join(flood_folder, flood_file)\n",
    "\n",
    "    if not os.path.exists(flood_path):\n",
    "        print(f\"File not found: {flood_file}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Load flood data\n",
    "        flood_gdf = gpd.read_file(flood_path)\n",
    "\n",
    "        # Filter flood geometries with water depth â‰¥ 0.3m\n",
    "        filtered_flood = flood_gdf[flood_gdf['water_depth'] >= 0.3]\n",
    "        if filtered_flood.empty:\n",
    "            print(f\"No valid geometries in {flood_file}\")\n",
    "            return None\n",
    "\n",
    "        # Ensure CRS alignment\n",
    "        filtered_flood = filtered_flood.to_crs(road_gdf.crs)\n",
    "\n",
    "        # Find intersecting roads\n",
    "        intersecting_roads = road_gdf[road_gdf.intersects(filtered_flood.unary_union)]\n",
    "        road_ids = intersecting_roads['road_id'].tolist()\n",
    "\n",
    "        # Save road IDs to a text file\n",
    "        output_file = os.path.join(output_folder, f\"{flood_file.replace('.shp', '')}_intersecting_roads.txt\")\n",
    "        with open(output_file, \"w\") as file:\n",
    "            for road_id in road_ids:\n",
    "                file.write(f\"{road_id}\\n\")\n",
    "\n",
    "        print(f\"Processed {flood_file}: {len(road_ids)} roads saved.\")\n",
    "        return len(road_ids)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {flood_file}: {e}\")\n",
    "        return None\n",
    "# Process files in parallel\n",
    "# Define the range of file indices (e.g., 73 flood scenarios)\n",
    "file_indices = range(73)\n",
    "\n",
    "# Parallel processing with Joblib\n",
    "results = Parallel(n_jobs=-1)(delayed(process_flood_file)(index) for index in file_indices)\n",
    "\n",
    "# Summary of processing\n",
    "total_processed = sum(1 for result in results if result is not None)\n",
    "print(f\"Total files processed: {total_processed}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdal_cartopy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
